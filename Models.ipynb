{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75dc48-68cb-4162-ae5f-d016fa6e0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a0a552-35a2-41dd-85db-ddee79367fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input png file in list \n",
    "#####################################################################################\n",
    "#______________________________INPUT require file here_______________________________\n",
    "#####################################################################################\n",
    "test_image_paths = [r'C:\\Users\\ethan\\Downloads\\target_images\\target_images\\line_1.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a508d-2d3d-4239-b600-1dd77e576f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CSV file containing labels\n",
    "labels_df = pd.read_csv(r'C:\\Users\\ethan\\Downloads\\alphabets_dataset\\alphabet_labels.csv')\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for index, row in labels_df.iterrows():\n",
    "    filename = row['file']\n",
    "    label = row['label']\n",
    "    img_path = os.path.join(r'C:\\Users\\ethan\\Downloads\\alphabets_dataset\\alphabet_images', filename)\n",
    "    img = load_img(img_path, color_mode='grayscale', target_size=(28, 28))\n",
    "    img_array = img_to_array(img)\n",
    "    images.append(img_array)\n",
    "    labels.append(label)\n",
    "\n",
    "images = np.array(images) / 255.0  # Normalizing pixel values between 0 and 1\n",
    "labels = np.array(labels)\n",
    "\n",
    "\n",
    "# Encoding string labels to integers\n",
    "label_Encoder = LabelEncoder()\n",
    "labels = label_Encoder.fit_transform(labels)\n",
    "\n",
    "# Checking for class imbalances\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "class_weights = {i: 1.0 / count for i, count in enumerate(counts)}\n",
    "\n",
    "# Splitting data for training and testing \n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshaping data for the CNN input\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test = x_test.reshape((-1, 28, 28, 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1155cc3-08f2-47b9-831c-a41525117323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a TF dataset\n",
    "def create_dataset(images, labels, batch_size=32, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=1024)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(x_train, y_train)\n",
    "test_dataset = create_dataset(x_test, y_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c50aa-8161-4a12-a79b-8e2058544a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model for character identification\n",
    "num_classes = len(np.unique(labels))\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])  \n",
    "\n",
    "# Learning rate scheduler\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                                 patience=3, min_lr=0.001)\n",
    "\n",
    "# Training the model with the dataset\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=20,\n",
    "                    validation_data=test_dataset,\n",
    "                    callbacks=[reduce_lr],\n",
    "                    class_weight=class_weights)\n",
    "\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e51c5-494c-43fd-a1a9-b6e191ff2986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Verifying image paths\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loading and preprocessing a test image\n",
    "def preprocess_test_image(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Image at path {img_path} could not be loaded.\")\n",
    "    return img\n",
    "\n",
    "# Slicing the test image into individual characters\n",
    "def slice_image_into_characters(img, char_width=28, char_height=28):\n",
    "    #L is a list of characters : 'X' if it identifies a letter and ' ' if it is just a whitespace\n",
    "    L=[]\n",
    "    characters = []\n",
    "    h, w = img.shape\n",
    "    last_x = 0\n",
    "    for y in range(0, h, char_height):\n",
    "        for x in range(0, w, char_width):\n",
    "            char = img[y:y + char_height, x:x + char_width]\n",
    "            if char.shape == (char_height, char_width):  # Ensure the character has the right dimensions\n",
    "                if np.mean(char) < 5:  # Assuming very low mean intensity indicates a space\n",
    "                    characters.append(' ')  # Represent space as a string ' '\n",
    "                else:\n",
    "                    char = char / 255.0  # Normalize pixel values\n",
    "                    char = char.reshape((char_height, char_width, 1))\n",
    "                    characters.append(char)\n",
    "                last_x = x\n",
    "    \n",
    "    #Identifying what is a character and what isnt\n",
    "    for char in characters:\n",
    "        if isinstance(char, str):\n",
    "            #print(char, end='')\n",
    "            L.append(char)\n",
    "        else:\n",
    "            #print('X', end='')  # Placeholder for non-string characters\n",
    "            L.append('X')\n",
    "    \n",
    "    return characters, L\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Process test images and spaces\n",
    "all_characters_processed = []\n",
    "\n",
    "for path in test_image_paths:\n",
    "    try:\n",
    "        img = preprocess_test_image(path)\n",
    "        characters,L = slice_image_into_characters(img)\n",
    "        all_characters_processed.extend(characters)\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "\n",
    "# Separate images and spaces\n",
    "images_to_predict = []\n",
    "for char in all_characters_processed:\n",
    "    if isinstance(char, np.ndarray):\n",
    "        images_to_predict.append(char)\n",
    "\n",
    "# Convert images to numpy array\n",
    "all_characters = np.array(images_to_predict)\n",
    "statement=''\n",
    "# Ensure the model is available as model\n",
    "if all_characters.shape[0] > 0:\n",
    "    # Make predictions\n",
    "    predictions = model.predict(all_characters)\n",
    "\n",
    "    # Map predictions to characters\n",
    "    def get_character_from_prediction(prediction):\n",
    "        classes = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz '  # Including space\n",
    "        return classes[np.argmax(prediction)]\n",
    "\n",
    "    predicted_characters = [get_character_from_prediction(pred) for pred in predictions]\n",
    "\n",
    "    # Print the predicted characters\n",
    "    \n",
    "    \n",
    "    printed_list=(''.join(predicted_characters))\n",
    "   #Compare our list of characters without spaces to list L which determines where the whitespaces go\n",
    "    index_L=index_pl=0\n",
    "    while index_L <(len(L)):\n",
    "       if L[index_L]==' ': \n",
    "           statement+=' '\n",
    "           index_L+=1\n",
    "       else: \n",
    "           statement+=printed_list[index_pl]\n",
    "           index_L+=1\n",
    "           index_pl+=1\n",
    "else:\n",
    "    print(\"No characters to predict.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2578fd2f-4e8f-48ef-b30a-acd351d7b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Loading data from CSV file\n",
    "df = pd.read_csv(r'C:\\Users\\ethan\\Downloads\\sentiment_analysis_dataset.csv')\n",
    "\n",
    "\n",
    "sentences = df['line'].str.lower().tolist()\n",
    "labels = df['sentiment'].tolist()\n",
    "\n",
    "# Converting labels to numerical values\n",
    "label_dict = {'Angry': 0, 'Happy': 1, 'Neutral': 2}\n",
    "labels = [label_dict[label] for label in labels]\n",
    "\n",
    "# Ensuring class balance\n",
    "class_counts = pd.Series(labels).value_counts()\n",
    "print(\"Class distribution before balancing:\", class_counts)\n",
    "\n",
    "# Tokenizing sentences\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=50, padding='post', truncating='post')\n",
    "\n",
    "# Calculating class weights\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Building the model for sentiment analysis\n",
    "model2 = tf.keras.Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=64, input_length=50),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#Compiling the model\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Training the model\n",
    "model2.fit(X_train, np.array(y_train), epochs=20, batch_size=32, validation_data=(X_val, np.array(y_val)), \n",
    "          class_weight=class_weights, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48da3d5e-51c3-4b18-80eb-e2c984f6f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Prediction\n",
    "\n",
    "test_sentences = [statement.lower()]\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=50, padding='post', truncating='post')\n",
    "predictions = model2.predict(padded_test_sequences)\n",
    "\n",
    "sentiments = ['Angry', 'Happy', 'Neutral']\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(sentiments[np.argmax(prediction)])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
